{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaoObv8TPDg9",
        "outputId": "9bbe92ed-0168-4872-f52a-13f691ab47b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oLCDHF_fXlp6"
      },
      "outputs": [],
      "source": [
        "word_label_dict = {}\n",
        "seen_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text=(\"Gus is running helping and caring organise a developed conference on Applications of Natural language processing. He keeps organising local Python meetups and several internal talks at his workplace.\")\n",
        "conference_help_doc = nlp(text)\n",
        "# print(conference_help_doc)\n",
        "for token in conference_help_doc:\n",
        "    if str(token) != str(token.lemma_):\n",
        "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ],
      "metadata": {
        "id": "XaIBBEagOXtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cf6389-dfcf-4866-e61a-b227ecd05fa2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  is : be\n",
            "             running : run\n",
            "             helping : help\n",
            "              caring : care\n",
            "           developed : develop\n",
            "             Natural : natural\n",
            "                  He : he\n",
            "               keeps : keep\n",
            "          organising : organise\n",
            "             meetups : meetup\n",
            "               talks : talk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "G1aDkYhGaNyL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def process_input(text):\n",
        "  text = text.lower()\n",
        "  sen_tok_list = []\n",
        "  wpt = WordPunctTokenizer()\n",
        "  sen_tok = wpt.tokenize(text)\n",
        "  sen_tok_list.append(sen_tok)\n",
        "  final_word_list = []\n",
        "  wnlem = WordNetLemmatizer()\n",
        "  for i in range(len(sen_tok_list)):\n",
        "    for j in range(len(sen_tok_list[i])):\n",
        "      sen_tok_list[i][j] = wnlem.lemmatize(sen_tok_list[i][j])\n",
        "  for i in range(len(sen_tok_list)):\n",
        "    temp_word_list = []\n",
        "    for j in range(len(sen_tok_list[i])):\n",
        "      if sen_tok_list[i][j].isalpha() == True and sen_tok_list[i][j].isnumeric() == False:\n",
        "        temp_word_list.append(sen_tok_list[i][j])\n",
        "    final_word_list.append(temp_word_list)\n",
        "  return final_word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RxZyi0s9Pqpl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def generate_final_word_list(file_path):\n",
        "  dataframe = pd.read_csv(file_path,sep = '\\t',header = None,names = [\"ID\",\"Sentence\",\"complex_phrase_start_index\",\"complex_phrase_end_index\",\"Complex_phrase\",\"num_native\",\"num_n_native\",\"native_complex\",\"non-native complex\",\"Binary values\",\"probability values\"])\n",
        "  dataframe[\"Sentence\"] = dataframe[\"Sentence\"].apply(lambda x : x.lower())\n",
        "  dataframe[\"Complex_phrase\"] = dataframe[\"Complex_phrase\"].apply(lambda x : x.lower())\n",
        "  dataframe[\"Num_words\"] = dataframe[\"Complex_phrase\"].apply(lambda x : len(x.split()))\n",
        "  dataframe = dataframe[dataframe[\"Num_words\"] == 1]\n",
        "  sentences_list = np.array(dataframe[\"Sentence\"])\n",
        "  labels = np.array(dataframe[\"Binary values\"])\n",
        "  complex_words = np.array(dataframe[\"Complex_phrase\"])\n",
        "  complex_word_dict = {complex_words[i]: labels[i] for i in range(len(labels))}\n",
        "  sen_tok_list = []\n",
        "  wpt = WordPunctTokenizer()\n",
        "  for sentence in sentences_list:\n",
        "    sen_tok = wpt.tokenize(sentence)\n",
        "    sen_tok_list.append(sen_tok)\n",
        "  wnlem = WordNetLemmatizer()\n",
        "  for i in range(len(sen_tok_list)):\n",
        "    for j in range(len(sen_tok_list[i])):\n",
        "      sen_tok_list[i][j] = wnlem.lemmatize(sen_tok_list[i][j])\n",
        "  final_word_list = []\n",
        "  for i in range(len(sen_tok_list)):\n",
        "    temp_word_list = []\n",
        "    for j in range(len(sen_tok_list[i])):\n",
        "      if sen_tok_list[i][j].isalpha() == True and sen_tok_list[i][j].isnumeric() == False:\n",
        "        temp_word_list.append(sen_tok_list[i][j])\n",
        "    final_word_list.append(temp_word_list)\n",
        "  for sentence in final_word_list:\n",
        "    for word in sentence:\n",
        "      if word not in seen_dict:\n",
        "        seen_dict[word] = 1\n",
        "        if word not in complex_word_dict:\n",
        "          word_label_dict[word] = 0\n",
        "        else:\n",
        "          word_label_dict[word] = complex_word_dict[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ui6VkmUXZUO1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "wnlem = WordNetLemmatizer()\n",
        "def complex_word_check(word):\n",
        "  if(word in string.punctuation):\n",
        "    return False\n",
        "  word = wnlem.lemmatize(word)\n",
        "  if word in word_label_dict:\n",
        "    return word_label_dict[word]\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGKIprYJoyQ6",
        "outputId": "279da5ad-f5de-480e-95f6-57ebdc5fd37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.9/dist-packages (3.0.3)\n",
            "Requirement already satisfied: regex>=2021.7.6 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (2022.10.31)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (6.1.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (1.0.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy>=6.1->wordfreq) (0.2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install wordfreq\n",
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUXnncn1IOW-",
        "outputId": "7bc2f8e2-6765-44b9-9abb-8d5ddf094954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pU-98TKiXTnL"
      },
      "outputs": [],
      "source": [
        "def get_top_k_predictions(input_string, k=5, tokenizer=tokenizer, model=model) -> str:\n",
        "\n",
        "    tokenized_inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
        "    outputs = model(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "    top_k_indices = tf.math.top_k(outputs.logits, k).indices[0].numpy()\n",
        "    decoded_output = tokenizer.batch_decode(top_k_indices)\n",
        "    mask_token = tokenizer.encode(tokenizer.mask_token)[1:-1]\n",
        "    mask_index = np.where(tokenized_inputs['input_ids'].numpy()[0]==mask_token)[0][0]\n",
        "\n",
        "    decoded_output_words = decoded_output[mask_index]\n",
        "\n",
        "    return decoded_output_words   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wordfreq import zipf_frequency\n",
        "def sentence_complexity(sentence):\n",
        "  wpt = WordPunctTokenizer()\n",
        "  tokens = wpt.tokenize(sentence)\n",
        "  complexity_sum = 0\n",
        "  num_tokens = len(tokens)\n",
        "  for i in range(num_tokens):\n",
        "    complexity_sum+= zipf_frequency(tokens[i], 'en')\n",
        "  return complexity_sum"
      ],
      "metadata": {
        "id": "Zdtyd-xDUbHN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "bert_similarity_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "ZNNnosWGB2A2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordfreq import zipf_frequency\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textstat\n",
        "def simplify_sentence(text):\n",
        "  text = text.lower()\n",
        "  wpt = WordPunctTokenizer()\n",
        "  tokens = wpt.tokenize(text)\n",
        "  complex_words = []\n",
        "  index_of_complex_word = []\n",
        "  for i in range(len(tokens)):\n",
        "    if(textstat.difficult_words(tokens[i]) == 1):\n",
        "      complex_words.append(tokens[i])\n",
        "      index_of_complex_word.append(i)\n",
        "  sentence1_transformed = bert_similarity_model.encode(text)\n",
        "  for j in range(len(complex_words)):\n",
        "    text_with_mask = text.replace(complex_words[j], '[MASK]')\n",
        "    predictedWords = get_top_k_predictions(input_string = text_with_mask)\n",
        "    predictedWords = predictedWords.split()\n",
        "    word = ''\n",
        "    similarity = []\n",
        "    for i in range(len(predictedWords)):\n",
        "      if(complex_words[j] == predictedWords[i]):\n",
        "        similarity.append(0)\n",
        "      simpler_text = text.replace(complex_words[j], predictedWords[i])\n",
        "      sentence2_transformed = bert_similarity_model.encode(simpler_text)\n",
        "      similarity_val = cosine_similarity([sentence1_transformed],[sentence2_transformed])[0][0]\n",
        "      similarity.append(similarity_val)\n",
        "    if(len(similarity) == 0):\n",
        "      tokens[index_of_complex_word[j]] = complex_words[j]\n",
        "    else:\n",
        "      tokens[index_of_complex_word[j]] = predictedWords[similarity.index(max(similarity))]\n",
        "  sentence = \"\"\n",
        "  for token in tokens:\n",
        "    sentence += token + \" \"\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "rsE-yb-eCE-h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text2 = \"Draconian laws made everybody miserable.\"\n",
        "text3 = 'The door to his residence was locked'\n",
        "text4 = 'Agree to my demands or i will intimidate you.'\n",
        "\n",
        "simplified_text2 = simplify_sentence(text2)\n",
        "simplified_text3 = simplify_sentence(text3)\n",
        "simplified_text4 = simplify_sentence(text4)\n",
        "\n",
        "print(\"Original text is: {} \\nSimplified text is: {}\".format(text4,simplified_text4))\n",
        "print(\"Simplicity of original text is: {} \\nSimplicity of simplified text is : {}\".format(sentence_complexity(text4),sentence_complexity(simplified_text4)))\n",
        "print(\"Original text is: {} \\nSimplified text is: {}\".format(text3,simplified_text3))\n",
        "print(\"Simplicity of original text is: {} \\nSimplicity of simplified text is : {}\".format(sentence_complexity(text3),sentence_complexity(simplified_text3)))\n",
        "print(\"Original text is: {} \\nSimplified text is: {}\".format(text2,simplified_text2))\n",
        "print(\"Simplicity of original text is: {} \\nSimplicity of simplified text is : {}\".format(sentence_complexity(text2),sentence_complexity(simplified_text2)))"
      ],
      "metadata": {
        "id": "LG-Hn71ReApm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5a0c44-0746-4a85-b98f-a58c0ba4483b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text is: Agree to my demands or i will intimidate you. \n",
            "Simplified text is: agree to my plan or i will torture you . \n",
            "Simplicity of original text is: 53.75 \n",
            "Simplicity of simplified text is : 55.47\n",
            "Original text is: The door to his residence was locked \n",
            "Simplified text is: the door to his apartment was locked \n",
            "Simplicity of original text is: 42.36 \n",
            "Simplicity of simplified text is : 42.65\n",
            "Original text is: Draconian laws made everybody miserable. \n",
            "Simplified text is: the laws made everybody insane . \n",
            "Simplicity of original text is: 22.6 \n",
            "Simplicity of simplified text is : 27.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\n",
        "simplified_text = simplify_sentence(input_text)\n",
        "print(\"Original text is: {} \\nSimplified text is: {}\".format(input_text,simplified_text))\n",
        "print(\"Simplicity of original text is: {} \\nSimplicity of simplified text is : {}\".format(sentence_complexity(input_text),sentence_complexity(simplified_text)))"
      ],
      "metadata": {
        "id": "vw0zxDzKV_3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFlMOxwLCbPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}