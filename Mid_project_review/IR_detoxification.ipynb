{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RkVLW3KXPYY8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/all_data.csv\")"
      ],
      "metadata": {
        "id": "s4oCkM3lPofM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y9MRYbxQujKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tN3nX-GGRkTz",
        "outputId": "3428228b-7056-49b4-95c6-bb50d350dacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              id                                       comment_text  split  \\\n",
              "0        1083994  He got his money... now he lies in wait till a...  train   \n",
              "1         650904  Mad dog will surely put the liberals in mental...  train   \n",
              "2        5902188  And Trump continues his lifelong cowardice by ...  train   \n",
              "3        7084460  \"while arresting a man for resisting arrest\".\\...   test   \n",
              "4        5410943     Tucker and Paul are both total bad ass mofo's.  train   \n",
              "...          ...                                                ...    ...   \n",
              "1999511  1018736  Another man shamming article. If white men did...  train   \n",
              "1999512   340016  \"no matter what is put in front of you regardi...  train   \n",
              "1999513   919629  The Democrat party aided and abetted by it's M...  train   \n",
              "1999514  5165492  I just don't find her a very good representati...  train   \n",
              "1999515  4984105  You know the Trump fanatics are trolling the G...  train   \n",
              "\n",
              "                          created_date  publication_id  parent_id  article_id  \\\n",
              "0        2017-03-06 15:21:53.675241+00              21        NaN      317120   \n",
              "1        2016-12-02 16:44:21.329535+00              21        NaN      154086   \n",
              "2        2017-09-05 19:05:32.341360+00              55        NaN      374342   \n",
              "3        2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
              "4        2017-06-14 05:08:21.997315+00              21        NaN      344096   \n",
              "...                                ...             ...        ...         ...   \n",
              "1999511  2017-02-20 07:20:49.964620+00              54        NaN      169202   \n",
              "1999512  2016-06-06 06:43:04.780968+00              21   339965.0      137961   \n",
              "1999513  2017-01-30 02:44:29.168863+00              54        NaN      164845   \n",
              "1999514  2017-04-22 18:42:02.442987+00              54        NaN      328877   \n",
              "1999515  2017-03-10 00:55:35.369198+00              54   807615.0      156960   \n",
              "\n",
              "           rating  funny  wow  ...  white  asian  latino  \\\n",
              "0        approved      0    0  ...    NaN    NaN     NaN   \n",
              "1        approved      0    0  ...    NaN    NaN     NaN   \n",
              "2        approved      1    0  ...    NaN    NaN     NaN   \n",
              "3        approved      0    0  ...    NaN    NaN     NaN   \n",
              "4        approved      0    0  ...    NaN    NaN     NaN   \n",
              "...           ...    ...  ...  ...    ...    ...     ...   \n",
              "1999511  approved      0    0  ...    0.8    0.0     0.0   \n",
              "1999512  approved      0    0  ...    0.0    0.0     0.0   \n",
              "1999513  rejected      0    1  ...    0.0    0.0     0.0   \n",
              "1999514  approved      1    0  ...    0.0    0.0     0.0   \n",
              "1999515  approved      1    0  ...    0.0    0.0     0.0   \n",
              "\n",
              "         other_race_or_ethnicity  physical_disability  \\\n",
              "0                            NaN                  NaN   \n",
              "1                            NaN                  NaN   \n",
              "2                            NaN                  NaN   \n",
              "3                            NaN                  NaN   \n",
              "4                            NaN                  NaN   \n",
              "...                          ...                  ...   \n",
              "1999511                      0.0             0.000000   \n",
              "1999512                      0.0             0.000000   \n",
              "1999513                      0.0             0.000000   \n",
              "1999514                      0.0             0.003717   \n",
              "1999515                      0.0             0.000000   \n",
              "\n",
              "         intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
              "0                                        NaN                            NaN   \n",
              "1                                        NaN                            NaN   \n",
              "2                                        NaN                            NaN   \n",
              "3                                        NaN                            NaN   \n",
              "4                                        NaN                            NaN   \n",
              "...                                      ...                            ...   \n",
              "1999511                                  0.0                            0.0   \n",
              "1999512                                  0.0                            0.0   \n",
              "1999513                                  0.0                            0.0   \n",
              "1999514                                  0.0                            0.0   \n",
              "1999515                                  0.0                            0.0   \n",
              "\n",
              "         other_disability  identity_annotator_count  toxicity_annotator_count  \n",
              "0                     NaN                         0                        67  \n",
              "1                     NaN                         0                        76  \n",
              "2                     NaN                         0                        63  \n",
              "3                     NaN                         0                        76  \n",
              "4                     NaN                         0                        80  \n",
              "...                   ...                       ...                       ...  \n",
              "1999511           0.00000                        10                        10  \n",
              "1999512           0.00000                        10                        10  \n",
              "1999513           0.00000                        11                        10  \n",
              "1999514           0.00000                       269                        10  \n",
              "1999515           0.00064                      1562                        10  \n",
              "\n",
              "[1999516 rows x 46 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69dc92d7-98e0-4d0a-ac1c-acd0b4b54eab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>split</th>\n",
              "      <th>created_date</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>asian</th>\n",
              "      <th>latino</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1083994</td>\n",
              "      <td>He got his money... now he lies in wait till a...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-03-06 15:21:53.675241+00</td>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>317120</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>650904</td>\n",
              "      <td>Mad dog will surely put the liberals in mental...</td>\n",
              "      <td>train</td>\n",
              "      <td>2016-12-02 16:44:21.329535+00</td>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>154086</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5902188</td>\n",
              "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-09-05 19:05:32.341360+00</td>\n",
              "      <td>55</td>\n",
              "      <td>NaN</td>\n",
              "      <td>374342</td>\n",
              "      <td>approved</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7084460</td>\n",
              "      <td>\"while arresting a man for resisting arrest\".\\...</td>\n",
              "      <td>test</td>\n",
              "      <td>2016-11-01 16:53:33.561631+00</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>149218</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5410943</td>\n",
              "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-06-14 05:08:21.997315+00</td>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>344096</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999511</th>\n",
              "      <td>1018736</td>\n",
              "      <td>Another man shamming article. If white men did...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-02-20 07:20:49.964620+00</td>\n",
              "      <td>54</td>\n",
              "      <td>NaN</td>\n",
              "      <td>169202</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999512</th>\n",
              "      <td>340016</td>\n",
              "      <td>\"no matter what is put in front of you regardi...</td>\n",
              "      <td>train</td>\n",
              "      <td>2016-06-06 06:43:04.780968+00</td>\n",
              "      <td>21</td>\n",
              "      <td>339965.0</td>\n",
              "      <td>137961</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999513</th>\n",
              "      <td>919629</td>\n",
              "      <td>The Democrat party aided and abetted by it's M...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-01-30 02:44:29.168863+00</td>\n",
              "      <td>54</td>\n",
              "      <td>NaN</td>\n",
              "      <td>164845</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999514</th>\n",
              "      <td>5165492</td>\n",
              "      <td>I just don't find her a very good representati...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-04-22 18:42:02.442987+00</td>\n",
              "      <td>54</td>\n",
              "      <td>NaN</td>\n",
              "      <td>328877</td>\n",
              "      <td>approved</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>269</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999515</th>\n",
              "      <td>4984105</td>\n",
              "      <td>You know the Trump fanatics are trolling the G...</td>\n",
              "      <td>train</td>\n",
              "      <td>2017-03-10 00:55:35.369198+00</td>\n",
              "      <td>54</td>\n",
              "      <td>807615.0</td>\n",
              "      <td>156960</td>\n",
              "      <td>approved</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00064</td>\n",
              "      <td>1562</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1999516 rows × 46 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69dc92d7-98e0-4d0a-ac1c-acd0b4b54eab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69dc92d7-98e0-4d0a-ac1c-acd0b4b54eab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69dc92d7-98e0-4d0a-ac1c-acd0b4b54eab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzaGp_3oRrJd",
        "outputId": "eb350bc1-4a3f-45e9-cf32-1c730839eab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1999516"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "6GeOzqxiS8ji",
        "outputId": "4c8f4157-5a20-4022-c589-4f9c21812261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  publication_id     parent_id    article_id         funny  \\\n",
              "count  1.999516e+06    1.999516e+06  1.134709e+06  1.999516e+06  1.999516e+06   \n",
              "mean   4.065400e+06    4.988997e+01  3.715138e+06  2.810257e+05  2.776687e-01   \n",
              "std    2.527563e+06    2.771895e+01  2.451507e+06  1.040778e+05  1.054819e+00   \n",
              "min    5.984800e+04    2.000000e+00  6.100600e+04  2.006000e+03  0.000000e+00   \n",
              "25%    8.565798e+05    2.100000e+01  7.930110e+05  1.600038e+05  0.000000e+00   \n",
              "50%    5.340220e+06    5.400000e+01  5.217531e+06  3.319250e+05  0.000000e+00   \n",
              "75%    5.955782e+06    5.400000e+01  5.774684e+06  3.662270e+05  0.000000e+00   \n",
              "max    7.194640e+06    1.150000e+02  6.333965e+06  3.995440e+05  1.020000e+02   \n",
              "\n",
              "                wow           sad         likes      disagree      toxicity  \\\n",
              "count  1.999516e+06  1.999516e+06  1.999516e+06  1.999516e+06  1.999516e+06   \n",
              "mean   4.437174e-02  1.089289e-01  2.441188e+00  5.808151e-01  1.029241e-01   \n",
              "std    2.458644e-01  4.555570e-01  4.712994e+00  1.854332e+00  1.970386e-01   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "50%    0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
              "75%    0.000000e+00  0.000000e+00  3.000000e+00  0.000000e+00  1.666667e-01   \n",
              "max    2.100000e+01  3.100000e+01  3.000000e+02  1.870000e+02  1.000000e+00   \n",
              "\n",
              "       ...          white          asian         latino  \\\n",
              "count  ...  448000.000000  448000.000000  448000.000000   \n",
              "mean   ...       0.056534       0.011886       0.006151   \n",
              "std    ...       0.215175       0.086906       0.058828   \n",
              "min    ...       0.000000       0.000000       0.000000   \n",
              "25%    ...       0.000000       0.000000       0.000000   \n",
              "50%    ...       0.000000       0.000000       0.000000   \n",
              "75%    ...       0.000000       0.000000       0.000000   \n",
              "max    ...       1.000000       1.000000       1.000000   \n",
              "\n",
              "       other_race_or_ethnicity  physical_disability  \\\n",
              "count            448000.000000        448000.000000   \n",
              "mean                  0.008158             0.001351   \n",
              "std                   0.042429             0.017461   \n",
              "min                   0.000000             0.000000   \n",
              "25%                   0.000000             0.000000   \n",
              "50%                   0.000000             0.000000   \n",
              "75%                   0.000000             0.000000   \n",
              "max                   1.000000             1.000000   \n",
              "\n",
              "       intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
              "count                        448000.000000                  448000.000000   \n",
              "mean                              0.001117                       0.012068   \n",
              "std                               0.016391                       0.089072   \n",
              "min                               0.000000                       0.000000   \n",
              "25%                               0.000000                       0.000000   \n",
              "50%                               0.000000                       0.000000   \n",
              "75%                               0.000000                       0.000000   \n",
              "max                               1.000000                       1.000000   \n",
              "\n",
              "       other_disability  identity_annotator_count  toxicity_annotator_count  \n",
              "count     448000.000000              1.999516e+06              1.999516e+06  \n",
              "mean           0.001219              1.431667e+00              8.775720e+00  \n",
              "std            0.014114              1.763593e+01              4.331605e+01  \n",
              "min            0.000000              0.000000e+00              3.000000e+00  \n",
              "25%            0.000000              0.000000e+00              4.000000e+00  \n",
              "50%            0.000000              0.000000e+00              4.000000e+00  \n",
              "75%            0.000000              0.000000e+00              6.000000e+00  \n",
              "max            0.600000              1.866000e+03              4.936000e+03  \n",
              "\n",
              "[8 rows x 42 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-430f4d56-478c-456d-92fd-53d46c705404\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>asian</th>\n",
              "      <th>latino</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.134709e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>448000.000000</td>\n",
              "      <td>1.999516e+06</td>\n",
              "      <td>1.999516e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.065400e+06</td>\n",
              "      <td>4.988997e+01</td>\n",
              "      <td>3.715138e+06</td>\n",
              "      <td>2.810257e+05</td>\n",
              "      <td>2.776687e-01</td>\n",
              "      <td>4.437174e-02</td>\n",
              "      <td>1.089289e-01</td>\n",
              "      <td>2.441188e+00</td>\n",
              "      <td>5.808151e-01</td>\n",
              "      <td>1.029241e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056534</td>\n",
              "      <td>0.011886</td>\n",
              "      <td>0.006151</td>\n",
              "      <td>0.008158</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.012068</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>1.431667e+00</td>\n",
              "      <td>8.775720e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.527563e+06</td>\n",
              "      <td>2.771895e+01</td>\n",
              "      <td>2.451507e+06</td>\n",
              "      <td>1.040778e+05</td>\n",
              "      <td>1.054819e+00</td>\n",
              "      <td>2.458644e-01</td>\n",
              "      <td>4.555570e-01</td>\n",
              "      <td>4.712994e+00</td>\n",
              "      <td>1.854332e+00</td>\n",
              "      <td>1.970386e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>0.215175</td>\n",
              "      <td>0.086906</td>\n",
              "      <td>0.058828</td>\n",
              "      <td>0.042429</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.016391</td>\n",
              "      <td>0.089072</td>\n",
              "      <td>0.014114</td>\n",
              "      <td>1.763593e+01</td>\n",
              "      <td>4.331605e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.984800e+04</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>6.100600e+04</td>\n",
              "      <td>2.006000e+03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.565798e+05</td>\n",
              "      <td>2.100000e+01</td>\n",
              "      <td>7.930110e+05</td>\n",
              "      <td>1.600038e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.340220e+06</td>\n",
              "      <td>5.400000e+01</td>\n",
              "      <td>5.217531e+06</td>\n",
              "      <td>3.319250e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.955782e+06</td>\n",
              "      <td>5.400000e+01</td>\n",
              "      <td>5.774684e+06</td>\n",
              "      <td>3.662270e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.666667e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.194640e+06</td>\n",
              "      <td>1.150000e+02</td>\n",
              "      <td>6.333965e+06</td>\n",
              "      <td>3.995440e+05</td>\n",
              "      <td>1.020000e+02</td>\n",
              "      <td>2.100000e+01</td>\n",
              "      <td>3.100000e+01</td>\n",
              "      <td>3.000000e+02</td>\n",
              "      <td>1.870000e+02</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.866000e+03</td>\n",
              "      <td>4.936000e+03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 42 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-430f4d56-478c-456d-92fd-53d46c705404')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-430f4d56-478c-456d-92fd-53d46c705404 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-430f4d56-478c-456d-92fd-53d46c705404');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UsCt18uTiEa",
        "outputId": "77669691-9303-4ec0-af05-e5d1f9e41b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'comment_text', 'split', 'created_date', 'publication_id',\n",
              "       'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
              "       'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
              "       'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender',\n",
              "       'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
              "       'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
              "       'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
              "       'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
              "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
              "       'other_disability', 'identity_annotator_count',\n",
              "       'toxicity_annotator_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data[\"toxicity\"].to_numpy().tolist()"
      ],
      "metadata": {
        "id": "U3XkOZqhUZvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[\"comment_text\"].to_numpy().tolist()"
      ],
      "metadata": {
        "id": "4g1WlJPaUeNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X) , type(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uymq9bw9UkaF",
        "outputId": "228f1d79-3558-40dc-91bc-b39e7b528355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, list)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del data[\"parent_id\"]\n",
        "del data[\"id\"]\n",
        "del data[\"split\"]\n",
        "del data[\"created_date\"]\n",
        "del data[\"publication_id\"]\n",
        "del data[\"article_id\"]\n",
        "del data[\"rating\"]"
      ],
      "metadata": {
        "id": "6ORzYApeTrDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing data\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdvbFtSrSrzM",
        "outputId": "4d082716-7b49-467f-bbf8-4b588e8aec08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pre_process_data(main , flag = False):\n",
        "\n",
        "    if (flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            Original Data\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" , main[0])\n",
        "        print(\"2 :\" ,main[1])\n",
        "        print(\"3 :\" ,main[2])\n",
        "        print(\"4 :\" ,main[3])\n",
        "        print(\"5 :\" ,main[4])\n",
        "\n",
        "\n",
        "    # Step 1 - lowercasing\n",
        "\n",
        "    lower_cased_data = []\n",
        "    for sent in main:\n",
        "        # if(type(sent) == float):\n",
        "        #     print(sent)\n",
        "        #     break\n",
        "        lower_cased_data.append(sent.lower())\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Lower Casing\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,lower_cased_data[0])\n",
        "        print(\"2 :\" ,lower_cased_data[1])\n",
        "        print(\"3 :\" ,lower_cased_data[2])\n",
        "        print(\"4 :\" ,lower_cased_data[3])\n",
        "        print(\"5 :\" ,lower_cased_data[4])\n",
        "\n",
        "    # Step 2 - Tokenization\n",
        "\n",
        "    token = WhitespaceTokenizer()\n",
        "\n",
        "    tokenized_data = []\n",
        "    for sent in lower_cased_data:\n",
        "        tokenized_data.append(token.tokenize(sent))\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Toeknization\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,tokenized_data[0])\n",
        "        print(\"2 :\" ,tokenized_data[1])\n",
        "        print(\"3 :\" ,tokenized_data[2])\n",
        "        print(\"4 :\" ,tokenized_data[3])\n",
        "        print(\"5 :\" ,tokenized_data[4])\n",
        "\n",
        "    \n",
        "    # Step 3 - removing stop words\n",
        "\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    no_stop_data = []\n",
        "\n",
        "    for li in tokenized_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            if (word not in stopWords):\n",
        "                temp.append(word)\n",
        "        no_stop_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Stop Words removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,no_stop_data[0])\n",
        "        print(\"2 :\" ,no_stop_data[1])\n",
        "        print(\"3 :\" ,no_stop_data[2])\n",
        "        print(\"4 :\" ,no_stop_data[3])\n",
        "        print(\"5 :\" ,no_stop_data[4])\n",
        "\n",
        "    # Step 4 - removing punctuations\n",
        "\n",
        "    puncts = string.punctuation\n",
        "    no_punc_data = []\n",
        "\n",
        "    for li in no_stop_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            temp_word = \"\"\n",
        "            for alp in word:\n",
        "                if (alp not in puncts):\n",
        "                    temp_word += alp\n",
        "            temp.append(temp_word)\n",
        "        no_punc_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Punctuations removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,no_punc_data[0])\n",
        "        print(\"2 :\" ,no_punc_data[1])\n",
        "        print(\"3 :\" ,no_punc_data[2])\n",
        "        print(\"4 :\" ,no_punc_data[3])\n",
        "        print(\"5 :\" ,no_punc_data[4])\n",
        "\n",
        "    # Step 5 - Removing white spaces\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    for li in no_punc_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            if (word != ''):\n",
        "                temp.append(word)\n",
        "        final_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After White Space removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,final_data[0])\n",
        "        print(\"2 :\" ,final_data[1])\n",
        "        print(\"3 :\" ,final_data[2])\n",
        "        print(\"4 :\" ,final_data[3])\n",
        "        print(\"5 :\" ,final_data[4])\n",
        "\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "CfWEnpQiRty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis = 0 , inplace = True)"
      ],
      "metadata": {
        "id": "EWOufDJcWxvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JraRw_lAW39x",
        "outputId": "37007bd2-82d0-46b8-d936-80f118d6c9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "448000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CaQ0UvIXGmg",
        "outputId": "8af72164-9e9e-4578-9c9d-da321a77f89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_pre_processed = pre_process_data(X)"
      ],
      "metadata": {
        "id": "AChJEqk2VCGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_pre_processed"
      ],
      "metadata": {
        "id": "5vwn3t-qVhiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to join the tokenized inputs first\n",
        "X_final = []\n",
        "for i in range(len(X)):\n",
        "    inp = X[i]\n",
        "    X_final.append(' '.join(inp))"
      ],
      "metadata": {
        "id": "hzkz9AGLcWVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_final"
      ],
      "metadata": {
        "id": "z7nLnxdTcr7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "bkBdOhbkXmZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train , Y_test = train_test_split(X,Y)"
      ],
      "metadata": {
        "id": "owuWUEUEYHfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "X_train = vectorizer.transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "fKC2Ve8vYOv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cniov2Nub_W5",
        "outputId": "7fe18286-df3e-4091-edb9-84972f76bfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Convert your textual data to a list of lists of words\n",
        "text_data = [sentence.split() for sentence in X]\n",
        "\n",
        "# Train a Word2Vec model on your text data\n",
        "model = Word2Vec(sentences=text_data, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert your text data into vectors using the Word2Vec model\n",
        "vectors = []\n",
        "for sentence in text_data:\n",
        "    sentence_vector = np.zeros(100)\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            word_vector = model.wv[word]\n",
        "            sentence_vector += word_vector\n",
        "        except KeyError:\n",
        "            pass\n",
        "    vectors.append(sentence_vector)\n"
      ],
      "metadata": {
        "id": "wINjCMnVhHdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Ylw42shLgK",
        "outputId": "150747e4-cbc8-4127-8c7c-a35d0035baa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "448000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoLXTnePjz3y",
        "outputId": "adbbc090-9de6-45b5-d368-50b43e7633a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = pd.DataFrame(vectors)\n",
        "encodings.to_csv(\"encoding.cvs\")"
      ],
      "metadata": {
        "id": "7cG6uRky4Vs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"encoding.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1KIpefPK4euR",
        "outputId": "87f7a292-21b2-4b34-c95c-182ab54bc8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4035332f-3554-4a0f-a94b-f69a6ec50ce9\", \"encoding.csv\", 852346738)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"encoding.csv\")"
      ],
      "metadata": {
        "id": "evLiSAtR6roS",
        "outputId": "750345f7-fdaa-482c-d21c-ad44632eb80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_281f17c4-96a0-4426-82ed-dafe325637cb\", \"encoding.csv\", 852346738)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train , Y_test = train_test_split(vectors,Y)"
      ],
      "metadata": {
        "id": "O_enI8BTkw6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train an SVM model on the training data\n",
        "clf = SVC(kernel='linear', C=1, random_state=42)\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the SVM model\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "YB6naFvjlh_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "ucz46fpX433q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "# Predict the labels for the test data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the LR model\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQrpkb75FR_",
        "outputId": "49a8d9c9-20e5-47f9-8289-deab73f2c80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9180892857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_temp = []\n",
        "for val in Y_train:\n",
        "    if val>0.5:\n",
        "        Y_train_temp.append(1)\n",
        "    else:\n",
        "        Y_train_temp.append(0)\n",
        "Y_train = Y_train_temp"
      ],
      "metadata": {
        "id": "9kAZZ8smmkNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_temp = []\n",
        "for val in Y_test:\n",
        "    if val>0.5:\n",
        "        Y_test_temp.append(1)\n",
        "    else:\n",
        "        Y_test_temp.append(0)\n",
        "Y_test = Y_test_temp"
      ],
      "metadata": {
        "id": "11ZbJtBwmpxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c7yR6PEamx3",
        "outputId": "a2977181-5c67-48c9-9ce6-3a81e7e80992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "def classify_it(input_word , model , tokenizer):\n",
        "\n",
        "    # Tokenize the input sentence and convert it into a PyTorch tensor\n",
        "    input_ids = torch.tensor(tokenizer.encode(input_word)).unsqueeze(0)\n",
        "\n",
        "    # Perform the classification\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs[0].argmax(dim=1).item()\n",
        "\n",
        "    if predictions == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "0pvZe6dfnQMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starring out toxic words"
      ],
      "metadata": {
        "id": "5ZKp2twD1fzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_words = []"
      ],
      "metadata": {
        "id": "YnfkaDvb1jCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detoxify(sent , model , tokenizer):\n",
        "\n",
        "    # First we will check whether the sent is toxic or not using our SVM model\n",
        "\n",
        "    # if the sent is not toxic then just return the original sentence\n",
        "\n",
        "    # else\n",
        "    sent = sent.split(\" \")\n",
        "    detoxified_sentence = \"\"\n",
        "    for word in sent:\n",
        "        # temp_string = word + \" is a ball.\"\n",
        "        if(classify_toxicity(word) == 1):\n",
        "            # We found a toxic word\n",
        "            detoxified_sentence += '*'*len(word)\n",
        "        else:\n",
        "            detoxified_sentence += word\n",
        "        detoxified_sentence += \" \"\n",
        "    return detoxified_sentence"
      ],
      "metadata": {
        "id": "bxK4-nZ11lSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3KbSCHadA1T",
        "outputId": "59d0a389-f8eb-4f19-a4d1-1a6836f4fab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YWHy6bEwjsqG",
        "outputId": "ee1adb3d-97cd-45eb-a76b-faa3a5e383dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I just don't find her a very good representation of the transexual community. She just seems so self-absorbed & concerned with such superficial issues.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detoxify(sent , model , tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Q5z8a8VSfdZx",
        "outputId": "648a487e-d266-4028-88aa-b7f1d9e0b7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shut ** ***** '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MaTj7Ub6lP6_",
        "outputId": "f9296b5e-cf6e-4ea9-f23e-434b7d600b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I just don't find her a very good representation of the transexual community. She just seems so self-absorbed & concerned with such superficial issues.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"shut up bitch\""
      ],
      "metadata": {
        "id": "nnCsLWF7hfAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_it(\"bro\",model,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKjQzx60dI-A",
        "outputId": "e245802d-a146-4a65-ea2c-1e4b826d5dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_toxicity(\"transexual\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0nK_-5ukcCY",
        "outputId": "e22d2e6e-b763-49b7-dc16-760075738e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_toxicity(word):\n",
        "    # Load the pre-trained BERT model and tokenizer\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenize the input word\n",
        "    tokenized = tokenizer(word, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "    # Classify the word\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.softmax(outputs.logits, dim=1)\n",
        "        toxic_score = predictions[0][1].item()\n",
        "\n",
        "    # Return whether the word is toxic or not\n",
        "    if toxic_score >= 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "5wxF7VVNeCAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr =[]\n",
        "for word in sent.split(\" \"):\n",
        "    arr.append(classify_toxicity(word))\n",
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCP9EUwWhMGe",
        "outputId": "f3f85a69-64fe-4397-aad5-5c385c7e8872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "L9MrWjKPiVdE",
        "outputId": "92cc3be7-9f72-4786-8035-9c7d933033f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I just don't find her a very good representation of the transexual community. She just seems so self-absorbed & concerned with such superficial issues.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = data[\"comment_text\"][1999514]"
      ],
      "metadata": {
        "id": "9uojEttbhhL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxwQMDRth7vR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}