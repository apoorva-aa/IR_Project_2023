# -*- coding: utf-8 -*-
"""IR_detoxification_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ufRdfn8hBdJUYwh9zvmR4Jp0O8DCruMT
"""

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
from nltk.tokenize import WhitespaceTokenizer
from nltk.corpus import stopwords
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('stopwords')
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import torch
from transformers import BertForSequenceClassification, BertTokenizer
from gensim.models import Word2Vec


data = pd.read_csv("all_data.csv")

# from google.colab import drive
# drive.mount('/content/drive')

# data

# len(data)

# data.describe()

# data.columns

Y = data["toxicity"].to_numpy().tolist()

X = data["comment_text"].to_numpy().tolist()

# type(X) , type(Y)

del data["parent_id"]
del data["id"]
del data["split"]
del data["created_date"]
del data["publication_id"]
del data["article_id"]
del data["rating"]

# Pre-processing data


def pre_process_data(main , flag = False):

    if (flag):
        print("-----------------------------------------------")
        print ("            Original Data")
        print("-----------------------------------------------")
        print("1 :" , main[0])
        print("2 :" ,main[1])
        print("3 :" ,main[2])
        print("4 :" ,main[3])
        print("5 :" ,main[4])


    # Step 1 - lowercasing

    lower_cased_data = []
    for sent in main:
    #     if(type(sent) == float):
    #         print(sent)
    #         break
        lower_cased_data.append(sent.lower())

    if(flag):
        print("-----------------------------------------------")
        print ("            After Lower Casing")
        print("-----------------------------------------------")
        print("1 :" ,lower_cased_data[0])
        print("2 :" ,lower_cased_data[1])
        print("3 :" ,lower_cased_data[2])
        print("4 :" ,lower_cased_data[3])
        print("5 :" ,lower_cased_data[4])

    # Step 2 - Tokenization

    token = WhitespaceTokenizer()

    tokenized_data = []
    for sent in lower_cased_data:
        tokenized_data.append(token.tokenize(sent))

    if(flag):
        print("-----------------------------------------------")
        print ("            After Toeknization")
        print("-----------------------------------------------")
        print("1 :" ,tokenized_data[0])
        print("2 :" ,tokenized_data[1])
        print("3 :" ,tokenized_data[2])
        print("4 :" ,tokenized_data[3])
        print("5 :" ,tokenized_data[4])

    
    # Step 3 - removing stop words

    stopWords = set(stopwords.words('english'))
    no_stop_data = []

    for li in tokenized_data:
        temp = []
        for word in li:
            if (word not in stopWords):
                temp.append(word)
        no_stop_data.append(temp)

    if(flag):
        print("-----------------------------------------------")
        print ("            After Stop Words removal")
        print("-----------------------------------------------")
        print("1 :" ,no_stop_data[0])
        print("2 :" ,no_stop_data[1])
        print("3 :" ,no_stop_data[2])
        print("4 :" ,no_stop_data[3])
        print("5 :" ,no_stop_data[4])

    # Step 4 - removing punctuations

    puncts = string.punctuation
    no_punc_data = []

    for li in no_stop_data:
        temp = []
        for word in li:
            temp_word = ""
            for alp in word:
                if (alp not in puncts):
                    temp_word += alp
            temp.append(temp_word)
        no_punc_data.append(temp)

    if(flag):
        print("-----------------------------------------------")
        print ("            After Punctuations removal")
        print("-----------------------------------------------")
        print("1 :" ,no_punc_data[0])
        print("2 :" ,no_punc_data[1])
        print("3 :" ,no_punc_data[2])
        print("4 :" ,no_punc_data[3])
        print("5 :" ,no_punc_data[4])

    # Step 5 - Removing white spaces

    final_data = []

    for li in no_punc_data:
        temp = []
        for word in li:
            if (word != ''):
                temp.append(word)
        final_data.append(temp)

    if(flag):
        print("-----------------------------------------------")
        print ("            After White Space removal")
        print("-----------------------------------------------")
        print("1 :" ,final_data[0])
        print("2 :" ,final_data[1])
        print("3 :" ,final_data[2])
        print("4 :" ,final_data[3])
        print("5 :" ,final_data[4])

    return final_data

data.dropna(axis = 0 , inplace = True)

Y = data["toxicity"].to_numpy().tolist()

X = data["comment_text"].to_numpy().tolist()
for sent in X:
    print(type(sent))
    break

X_pre_processed = pre_process_data(X)

X = X_pre_processed

X_final = []
for i in range(len(X)):
    inp = X[i]
    X_final.append(' '.join(inp))

X = X_final



X_train, X_test, Y_train , Y_test = train_test_split(X,Y)

vectorizer = CountVectorizer()
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)

text_data = [sentence.split() for sentence in X]

model = Word2Vec(sentences=text_data, window=5, min_count=1, workers=4)

vectors = []
for sentence in text_data:
    sentence_vector = np.zeros(100)
    for word in sentence:
        try:
            word_vector = model.wv[word]
            sentence_vector += word_vector
        except KeyError:
            pass
    vectors.append(sentence_vector)

X_train, X_test, Y_train , Y_test = train_test_split(vectors,Y)


Y_train_temp = []
for val in Y_train:
    if val>0.5:
        Y_train_temp.append(1)
    else:
        Y_train_temp.append(0)
Y_train = Y_train_temp

Y_test_temp = []
for val in Y_test:
    if val>0.5:
        Y_test_temp.append(1)
    else:
        Y_test_temp.append(0)
Y_test = Y_test_temp

logreg = LogisticRegression()
logreg.fit(X_train, Y_train)
# Predict the labels for the test data
y_pred = logreg.predict(X_test)

# Calculate the accuracy of the LR model
# accuracy = accuracy_score(Y_test, y_pred)
# print("Accuracy:", accuracy)

# !pip install transformers


def classify_it(input_word , model , tokenizer):

    # Tokenize the input sentence and convert it into a PyTorch tensor
    input_ids = torch.tensor(tokenizer.encode(input_word)).unsqueeze(0)

    # Perform the classification
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids)
        predictions = outputs[0].argmax(dim=1).item()

    if predictions == 0:
        return 0
    else:
        return 1

def classify_toxicity(word):
    # Load the pre-trained BERT model and tokenizer
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # Tokenize the input word
    tokenized = tokenizer(word, padding=True, truncation=True, max_length=128, return_tensors="pt")
    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]

    # Classify the word
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        predictions = torch.softmax(outputs.logits, dim=1)
        toxic_score = predictions[0][1].item()

    # Return whether the word is toxic or not
    if toxic_score >= 0.5:
        return 1
    else:
        return 0

"""Starring out toxic words"""

toxic_words = []

def detoxify(sent , model , tokenizer):

    # First we will check whether the sent is toxic or not using our SVM model

    # if the sent is not toxic then just return the original sentence

    # else
    sent = sent.split(" ")
    detoxified_sentence = ""
    for word in sent:
        # temp_string = word + " is a ball."
        if(classify_toxicity(word) == 1):
            # We found a toxic word
            detoxified_sentence += '*'*len(word)
        else:
            detoxified_sentence += word
        detoxified_sentence += " "
    return detoxified_sentence

# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# sent

# detoxify(sent , model , tokenizer)

# sent

# sent = "shut up bitch"

# classify_it("bro",model,tokenizer)

# classify_toxicity("transexual")



# arr =[]
# for word in sent.split(" "):
#     arr.append(classify_toxicity(word))
# arr

# sent

# sent = data["comment_text"][58]

# sent
print("reached")
def main_function(sent):
    if(classify_toxicity(sent) == 0):
        # The sentence is not toxic so just return the sentence itself
        return sent
    else:
        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        return detoxify(sent,model,tokenizer)

