{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "XpF6qOpf_GPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "UKN4buJNI-LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "VViBGoYsJAck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "uRWHW4EaJDnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import math"
      ],
      "metadata": {
        "id": "rT_zD-nW_3pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase(context,sentence):\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print (\"device \",device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    text = \"paraphrase: \"+ sentence + \" </s>\"\n",
        "\n",
        "    encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\n",
        "    input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "    model.eval()\n",
        "\n",
        "    diverse_beam_outputs = model.generate(\n",
        "        input_ids=input_ids,attention_mask=attention_mask,\n",
        "        max_length=128,\n",
        "        early_stopping=True,\n",
        "        num_beams=5,\n",
        "        num_beam_groups = 5,\n",
        "        num_return_sequences=5,\n",
        "        diversity_penalty = 0.70\n",
        "    )\n",
        "\n",
        "    for beam_output in diverse_beam_outputs:\n",
        "        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "        sentences.append(sent)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "38559Cq9JHu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "  # function to get the similarity score between two words\n",
        "def get_similarity(word1, word2):\n",
        "    # create Doc objects for each word\n",
        "    doc1 = nlp(word1)\n",
        "    doc2 = nlp(word2)\n",
        "\n",
        "    # get similarity score between the two Doc objects\n",
        "    similarity = doc1.similarity(doc2)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "FAxbakpJ_TK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"toxic_words.txt\", \"r\")\n",
        "toxic_words = file.read()\n",
        "file.close()\n",
        "\n",
        "toxic_words_list = toxic_words.split()"
      ],
      "metadata": {
        "id": "6R0jI6yF8YV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"neg_words.txt\", \"r\")\n",
        "neg_words = file.read()\n",
        "file.close()\n",
        "\n",
        "neg_words_list = neg_words.split()"
      ],
      "metadata": {
        "id": "rERbGm8a9YpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detoxify(sent):\n",
        "\n",
        "    sent = sent.lower()\n",
        "    sent_ = sent.split(\" \")\n",
        "    detoxified_sentence = \"\"\n",
        "    for index in range(len(sent_)):\n",
        "        word = sent_[index]\n",
        "        if word in toxic_words_list:\n",
        "          # print(\"toxic word found\", word)\n",
        "          sim = []\n",
        "          for i in range(len(neg_words_list)):\n",
        "            syn_word = neg_words_list[i]\n",
        "            val = get_similarity(syn_word, word)\n",
        "            sim.append(val)\n",
        "\n",
        "          similarities = np.array(sim)\n",
        "          idx = np.argmax(similarities) \n",
        "          sent_[index] = neg_words_list[idx]\n",
        "          \n",
        "    detoxified_sentence = ' '.join(sent_)\n",
        "    return detoxified_sentence"
      ],
      "metadata": {
        "id": "wCbyMsQ79R-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detoxification(toxic_sentence):\n",
        "  detoxified_sentence = detoxify(toxic_sentence)\n",
        "  print('detoxified sentence:', detoxified_sentence)\n",
        "  sentences = paraphrase(toxic_sentence, detoxified_sentence)\n",
        "  print('final_output_sentences:', sentences)"
      ],
      "metadata": {
        "id": "ukcs36qWJyno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}