{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva-aa/IR_Project_2023/blob/main/Lexical_text_simplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install wordfreq\n",
        "!pip install textstat"
      ],
      "metadata": {
        "id": "HfaDvrfBcnBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f89f31-1b07-47d9-d645-8d6243db1423"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=254ab2705c4f83c81d1d0d765961ba5dc42b8d69b64f704fc53f4f93a765930c\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.98\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.0.3-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (1.0.5)\n",
            "Requirement already satisfied: regex>=2021.7.6 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (2022.10.31)\n",
            "Collecting ftfy>=6.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.9/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy>=6.1->wordfreq) (0.2.6)\n",
            "Installing collected packages: ftfy, wordfreq\n",
            "Successfully installed ftfy-6.1.1 wordfreq-3.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "from wordfreq import zipf_frequency\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textstat\n",
        "import gensim.downloader as api\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "ISr7s03_cqm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0344f0d9-ed77-45bb-e1c5-ea5af51920be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "bert_similarity_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "synonym_model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "Elyg0zqUd37g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115149c8-1d4a-435f-8f2f-3ce6ed26bef9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKldE1XduLVN",
        "outputId": "d371bc68-88be-43e9-aabd-451ab1bb1990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import itertools\n",
        "from nltk.corpus import wordnet\n",
        "import gensim.downloader as api\n",
        "\n",
        "def sentence_complexity(sentence):\n",
        "    wpt = WordPunctTokenizer()\n",
        "    tokens = wpt.tokenize(sentence)\n",
        "    complexity_sum = 0\n",
        "    num_tokens = len(tokens)\n",
        "    for i in range(num_tokens):\n",
        "      complexity_sum+= zipf_frequency(tokens[i], 'en')\n",
        "    return complexity_sum\n",
        "def user_interface_function(text):\n",
        "  def get_top_k_predictions(input_string, k,tokenizer,model) -> str:\n",
        "    tokenized_inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
        "    outputs = model(tokenized_inputs[\"input_ids\"])\n",
        "    top_k_indices = tf.math.top_k(outputs.logits, 7).indices[0].numpy()\n",
        "    decoded_output = tokenizer.batch_decode(top_k_indices)\n",
        "    mask_token = tokenizer.encode(tokenizer.mask_token)[1:-1]\n",
        "    mask_index = np.where(tokenized_inputs['input_ids'].numpy()[0]==mask_token)[0][0]\n",
        "\n",
        "    decoded_output_words = decoded_output[mask_index]\n",
        "    return decoded_output_words    \n",
        "  def get_synonym(word):\n",
        "    if(word not in synonym_model.key_to_index):\n",
        "      return word\n",
        "    max_zip_frequency = 0\n",
        "    synonym_index = 0\n",
        "    synonym_list = synonym_model.most_similar(word, topn=10)\n",
        "    synonyms = [synonym[0] for synonym in synonym_list]\n",
        "    synonyms_similarity_measure = [synonym[1] for synonym in synonym_list]\n",
        "    remove_words_list = []\n",
        "    for i in range(len(synonyms)):\n",
        "      if(zipf_frequency(synonyms[i], 'en') < zipf_frequency(word, 'en')):\n",
        "        remove_words_list.append(i)\n",
        "    remove_words_list.sort(reverse=True)\n",
        "    for i in range(len(remove_words_list)):\n",
        "          synonyms.pop(remove_words_list[i])\n",
        "          synonyms_similarity_measure.pop(remove_words_list[i])\n",
        "    if(len(synonyms) == 0):\n",
        "      return word\n",
        "\n",
        "    print(\"synonyms\")\n",
        "    print(synonyms)\n",
        "    zipf_frequency_arr = []\n",
        "    for i in range(len(synonyms)):\n",
        "      zipf_frequency_arr.append(zipf_frequency(synonyms[i], 'en'))\n",
        "\n",
        "    indexed_synonyms_similarity_measure = list(enumerate(synonyms_similarity_measure))\n",
        "    sorted_sim_arr = sorted(indexed_synonyms_similarity_measure, key=lambda x: x[1],reverse = True)\n",
        "    indexed_zipf_frequency = list(enumerate(zipf_frequency_arr))\n",
        "    sorted_complex_arr = sorted(indexed_zipf_frequency, key=lambda x: x[1],reverse = True)\n",
        "    mydict_sim = {}\n",
        "    mydict_complex = {}\n",
        "    for i in range(len(sorted_complex_arr)):\n",
        "      mydict_sim[sorted_sim_arr[i][0]] = i + 1\n",
        "      mydict_complex[sorted_complex_arr[i][0]] = i + 1\n",
        "    min_rank = 100000000000000000\n",
        "    required_index = 0\n",
        "    print(mydict_sim)\n",
        "    print(mydict_complex)\n",
        "    for i in range(len(sorted_sim_arr)):\n",
        "      rank_val = mydict_sim[i] * mydict_complex[i]\n",
        "      print(\"{} {}\".format(rank_val,synonyms[i]))\n",
        "      if(min_rank > rank_val):\n",
        "        min_rank = rank_val\n",
        "        required_index = i\n",
        "      if(min_rank == rank_val and mydict_complex[i] < mydict_sim[i]):\n",
        "        required_index = i\n",
        "    return synonyms[required_index]\n",
        "  text = text.lower()\n",
        "  wpt = WordPunctTokenizer()\n",
        "  tokens = wpt.tokenize(text)\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  complex_words = []\n",
        "  index_of_complex_word = []\n",
        "  for i in range(len(tokens)):\n",
        "    if(textstat.difficult_words(tokens[i]) == 1):\n",
        "      complex_words.append(tokens[i])\n",
        "      index_of_complex_word.append(i)\n",
        "  sentence1_transformed = bert_similarity_model.encode(text)\n",
        "  best_2_word_list = []\n",
        "  if(len(tokens) < 6):\n",
        "    simpler_words = []\n",
        "    new_sentence = text\n",
        "    for j in range(len(complex_words)):\n",
        "       new_sentence = new_sentence.replace(complex_words[j],get_synonym(complex_words[j]))\n",
        "    return new_sentence\n",
        "  else:\n",
        "    for j in range(len(complex_words)):\n",
        "      if(index_of_complex_word[j] != (len(tokens) -1) and pos_tags[index_of_complex_word[j]][1] == \"JJ\" and pos_tags[index_of_complex_word[j] + 1][1] == \"NN\"):\n",
        "        best_2_word_list.append([get_synonym(complex_words[j])])\n",
        "      else:\n",
        "        text_with_mask = text.replace(complex_words[j], '[MASK]')\n",
        "        predictedWords = get_top_k_predictions(text_with_mask,10,tokenizer,model)\n",
        "        predictedWords = predictedWords.split()\n",
        "        word = ''\n",
        "        remove_words_list = []\n",
        "        for i in range(len(predictedWords)):\n",
        "          if(zipf_frequency(predictedWords[i], 'en') < zipf_frequency(complex_words[j], 'en')):\n",
        "            remove_words_list.append(i)\n",
        "          else:\n",
        "            for letter in predictedWords[i]:\n",
        "              if(letter in string.punctuation):\n",
        "                remove_words_list.append(i)\n",
        "                break\n",
        "        remove_words_list.sort(reverse=True)\n",
        "        for i in range(len(remove_words_list)):\n",
        "          predictedWords.pop(remove_words_list[i])\n",
        "        similarity = []\n",
        "        for i in range(len(predictedWords)):\n",
        "          if(complex_words[j] == predictedWords[i]):\n",
        "            similarity.append(0)\n",
        "          else:\n",
        "            simpler_text = text.replace(complex_words[j], predictedWords[i])\n",
        "            sentence2_transformed = bert_similarity_model.encode(simpler_text)\n",
        "            similarity_val = cosine_similarity([sentence1_transformed],[sentence2_transformed])[0][0]\n",
        "            similarity.append(similarity_val)\n",
        "        if(len(similarity) == 0):\n",
        "          best_2_word_list.append([complex_words[j]])\n",
        "        else:\n",
        "          best_word = predictedWords[similarity.index(max(similarity))]\n",
        "          predictedWords.pop(similarity.index(max(similarity)))\n",
        "          similarity.pop(similarity.index(max(similarity)))\n",
        "          if(len(predictedWords) != 0):\n",
        "            best_word_2 = predictedWords[similarity.index(max(similarity))]\n",
        "            best_2_word_list.append([best_word,best_word_2])\n",
        "          else:\n",
        "            best_2_word_list.append([complex_words[j]])\n",
        "    similarity_arr = []\n",
        "    possible_combinations = list(itertools.product(*best_2_word_list))\n",
        "    possible_combinations_complexity = []\n",
        "    simpler_text_arr = []\n",
        "    for i in range(len(possible_combinations)):\n",
        "      simpler_text = text\n",
        "      words_complexity = 0\n",
        "      for j in range(len(complex_words)):\n",
        "        simpler_text = simpler_text.replace(complex_words[j], possible_combinations[i][j])\n",
        "        words_complexity += zipf_frequency(possible_combinations[i][j], 'en')\n",
        "      simpler_text_arr.append(simpler_text)\n",
        "      possible_combinations_complexity.append(words_complexity)\n",
        "      sentence2_transformed = bert_similarity_model.encode(simpler_text)\n",
        "      similarity_val = cosine_similarity([sentence1_transformed],[sentence2_transformed])[0][0]\n",
        "      similarity_arr.append(similarity_val)\n",
        "    indexed_similarity_arr = list(enumerate(similarity_arr))\n",
        "    sorted_sim_arr = sorted(indexed_similarity_arr, key=lambda x: x[1],reverse = True)\n",
        "    indexed_possible_combinations_complexity = list(enumerate(possible_combinations_complexity))\n",
        "    sorted_complex_arr = sorted(indexed_possible_combinations_complexity, key=lambda x: x[1],reverse = True)\n",
        "    mydict_sim = {}\n",
        "    mydict_complex = {}\n",
        "    for i in range(len(sorted_sim_arr)):\n",
        "      mydict_sim[sorted_sim_arr[i][0]] = i + 1\n",
        "      mydict_complex[sorted_complex_arr[i][0]] = i + 1\n",
        "    min_rank = 100000000000000000\n",
        "    required_index = 0\n",
        "    for i in range(len(sorted_sim_arr)):\n",
        "      rank_val = mydict_sim[i] * mydict_complex[i]\n",
        "      if(min_rank > rank_val):\n",
        "        min_rank = rank_val\n",
        "        required_index = i\n",
        "      if(min_rank == rank_val and mydict_complex[i] < mydict_sim[i]):\n",
        "        required_index = i\n",
        "    new_sentence = text\n",
        "    for j in range(len(complex_words)):\n",
        "        new_sentence = new_sentence.replace(complex_words[j], possible_combinations[required_index][j])\n",
        "    return new_sentence"
      ],
      "metadata": {
        "id": "RqOcR9zHw9iD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The honourable man was accused of robbery.\"\n",
        "simplified_text = user_interface_function(input_text)"
      ],
      "metadata": {
        "id": "ZZ5ltVJoyjDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(simplified_text)\n",
        "print(sentence_complexity(input_text))\n",
        "print(sentence_complexity(simplified_text))"
      ],
      "metadata": {
        "id": "S7qbS8ZVHymm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}